{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D to 3D LSTM\n",
    "\n",
    "This is the first approach to try to estimate 3D points coordinates from 2D keypoints extracted with Openpose. Here I will build a simple LSTM to perform the task over the Panoptic Studio dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# Plotting utilities\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timeit import default_timer as timer\n",
    "import pyprind\n",
    "\n",
    "# Directory and file utilities\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = { 0:'one video', 1:'one signer', 2:'all signers'}\n",
    "MODE =  modes[0]\n",
    "video = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "Now I will define some functions in order to parse and organise the data, and later convert it to pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured as follows: in the dataset directory there are several folders, each folder corresponds to a recording; each of these folders contains a folder with the audio, folders with face, hands and body keypoints estimations for each frame, and a folder with the video recorded from different views.\n",
    "\n",
    "In this first approach I will be using the keypoints estimations. Every keypoint folder (face, hands or body) is organized the same way: it contains a json per frame of the video, which includes the 3D keypoints estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_keypoints` will go through each folder in the dataset directory and retrieve the face keypoints, the hands keypoints and the body keypoints. It will separate them into input (2D coordinates per joint per frame) and grountruth (third coordinate to estimate for each input 2D keypoint). \n",
    "The input will be of shape $([n videos, seq len, input size])$, where *seq_len* = number of frames, and *input_size* = face + hands + body keypoints, that is (70+(21+21)+26)x2 -multiplied by 2 because there are x and y coordinates-. The groundtruth (label) data will be of the same shape, except that the last dimension size will not be multiplied by 2 (there's only one coordinate to estimate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(data_path):\n",
    "    dataset = []\n",
    "    groundtruth = []\n",
    "    # Look over just the folders inside the directory\n",
    "    just_folders = filter(lambda x: isdir(join(data_path, x)), listdir(data_path))\n",
    "    for p in list(map(lambda x: join(data_path, x), just_folders)): \n",
    "        # Gets 2 list of n_frames lists, one for the 2D coordinates and one for the third coordinate.\n",
    "        # Each list of the n_frames lists contains, either the (x and y) or the z of each keypoint for the face(first line), hands(second), body(third).\n",
    "        # e.g. the first line will result in [[x1,y1,x2,y2...x70,y70]sub1...[x1,y1...x70,y70]subN], [[z1,z2...z70]sub1...[z1..z70]subN]\n",
    "        # Actually, as there will be two of each list above because there are two people en each video.\n",
    "        pose_2d, pose_3d = get_body(p)\n",
    "        \n",
    "        # Concatenates the coordinates for the face, hands and body on the last dimension, for each person.\n",
    "        vid_input_p1, vid_input_p2 = pose_2d\n",
    "        vid_labels_p1, vid_labels_p2 = pose_3d\n",
    "        \n",
    "        dataset.append(vid_input_p1)\n",
    "        dataset.append(vid_input_p2)\n",
    "        groundtruth.append(vid_labels_p1)\n",
    "        groundtruth.append(vid_labels_p2)\n",
    "        print(f'Completed folder {p}')\n",
    "    return dataset, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body json is organised a bit differently, inside each person object contains the *joints26* field with a list of 3D coordinates. But this list is structured as follows: *[x1,y1,z1,acc1,x2,y2,z2,acc2...]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(path):\n",
    "    body_2D_seq = ([], [])\n",
    "    body_3D_seq = ([], [])\n",
    "    paths = map(lambda x: join(path, 'hdPose3d_stage1_op25', x), sorted(listdir(join(path, 'hdPose3d_stage1_op25'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files:\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['bodies']:\n",
    "                if person['id'] != -1:\n",
    "                    x = person['joints26'][::4]\n",
    "                    y = person['joints26'][1::4]\n",
    "                    two_coord = [l[item] for item in range(len(x)) for l in [x,y]]\n",
    "                    third_coord = person['joints26'][2::4]\n",
    "                    body_2D_seq[person['id']].append(two_coord)\n",
    "                    body_3D_seq[person['id']].append(third_coord)\n",
    "                    i += 1\n",
    "                    pid = person['id']\n",
    "            if i<2:\n",
    "                body_2D_seq[1-pid].append(body_2D_seq[1-pid][-1])\n",
    "                body_3D_seq[1-pid].append(body_3D_seq[1-pid][-1])\n",
    "    print('Body completed.')\n",
    "    return body_2D_seq, body_3D_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl2\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl4\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl5\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl1\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl2\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl3\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl5\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl7\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl9\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl91\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl1\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl2\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl3\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl4\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl5\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl6\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl7\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl8\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl910\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl911\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl912\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl913\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../data/DB keypoints'\n",
    "dataset, groundtruth = get_keypoints(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_seq(dataset):\n",
    "    max_seq = max([len(x) for x in dataset])\n",
    "    seq_lengths = []\n",
    "    for seq in dataset:\n",
    "        seq_lengths.append(len(seq))\n",
    "        for i in range(max_seq-len(seq)):\n",
    "            seq.append([np.nan for j in range(len(seq[0]))])\n",
    "        \n",
    "    return max_seq, seq_lengths\n",
    "\n",
    "max_seq, seq_lengths = padding_seq(dataset)\n",
    "_, _ = padding_seq(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 8752, 52) (44, 8752, 26) (44,)\n"
     ]
    }
   ],
   "source": [
    "# From python lists to numpy ndarray.\n",
    "dataset = np.asarray(dataset)\n",
    "groundtruth = np.asarray(groundtruth)\n",
    "lengths = np.asarray(seq_lengths)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/body_data.npy', dataset)\n",
    "np.save('../../pickles/body_ground.npy', groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/body_lengths.npy', lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structuring\n",
    "Now let's convert the lists obtained to Pytorch tensors and organise them in train, validation and test datasets. \n",
    "First, I will define a padding function in order to make all the sequences of video frames the same length, so I can train the LSTM in batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from pickle\n",
    "Load keypoints from pre-saved pickle files instead of directly reading the jsons, can be found in below cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once they are numpy ndarrays I save the keypoints into pickle files for faster loading in later executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../../pickles/body_data.npy', allow_pickle=True)\n",
    "groundtruth = np.load('../../pickles/body_ground.npy', allow_pickle=True)\n",
    "lengths = np.load('../../pickles/body_lengths.npy', allow_pickle=True)\n",
    "max_seq = dataset.shape[1]\n",
    "print(max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8272 8272 8272 8272 8271 8271 8751 8751 5952 5952 7611 7611 6953 6953\n",
      " 6012 6012 8574 8574 5176 5176 8045 8045 7836 7836 7326 7326 7141 7141\n",
      " 8751 8751 5155 5155 8751 8751 8752 8752 8751 8751 8751 8751 8751 8751\n",
      " 7205 7205]\n"
     ]
    }
   ],
   "source": [
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517,)\n"
     ]
    }
   ],
   "source": [
    "if MODE == 'one video':\n",
    "    dataset, groundtruth = dataset[video], groundtruth[video]\n",
    "    chunks_d = np.split(dataset, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_g = np.split(groundtruth, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_d = tuple(np.expand_dims(c, axis=0) for c in chunks_d)\n",
    "    chunks_g = tuple(np.expand_dims(c, axis=0) for c in chunks_g)\n",
    "    \n",
    "    dataset = np.concatenate(chunks_d, axis=0)\n",
    "    groundtruth = np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.asarray([16 for i in range(lengths[video]//16)])\n",
    "    print(lengths.shape)\n",
    "\n",
    "elif MODE == 'one signer':\n",
    "    dataset, groundtruth, lengths = dataset[20::2], groundtruth[20::2], lengths[20::2]\n",
    "    print(lengths)\n",
    "    chunks_d, chunks_g = np.split(dataset, 16, axis=1), np.split(groundtruth, 16, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([547 if j*547<=lengths[i] \n",
    "                                    else (lengths[i]%547 if (j-1)*547<lengths[i] \n",
    "                                          else 0) for i in range(12)] for j in range(1,17)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "    \n",
    "elif MODE == 'all signers':\n",
    "    chunks_d, chunks_g = np.split(dataset, 8, axis=1), np.split(groundtruth, 8, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([1094 if j*1094<=lengths[i] \n",
    "                                    else (lengths[i]%1094 if (j-1)*1094<lengths[i] \n",
    "                                          else 0) for i in range(44)] for j in range(1,9)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 16, 52) (517, 16, 26) (517,)\n"
     ]
    }
   ],
   "source": [
    "# Clean all NaN videos\n",
    "dataset = np.delete(dataset, np.where(lengths==0), axis=0)\n",
    "groundtruth = np.delete(groundtruth, np.where(lengths==0), axis=0)\n",
    "lengths = np.delete(lengths, np.where(lengths==0), axis=0)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 16, 52) (517, 16, 26) (517,)\n"
     ]
    }
   ],
   "source": [
    "def align(tensor, coordinates=1):\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "\n",
    "align(dataset,2)\n",
    "align(groundtruth)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 16, 52) (517, 16, 26)\n"
     ]
    }
   ],
   "source": [
    "r = R.from_euler('y', 110, degrees=True)\n",
    "shapes = dataset.shape\n",
    "dataset = dataset.reshape(-1, 2)\n",
    "groundtruth = groundtruth.reshape(-1,1)\n",
    "xyz = np.concatenate((dataset, groundtruth), axis=1)\n",
    "xyz = r.apply(xyz)\n",
    "dataset, groundtruth = xyz[:, :2].reshape(shapes), xyz[:,2].reshape(shapes[0], shapes[1],26)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 2) (517,)\n"
     ]
    }
   ],
   "source": [
    "def norm_uniform(tensor, coordinates=1):\n",
    "    scale = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_scale = []\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "            subtensor[:] = np.divide(subtensor, max_value[j]-center[j])\n",
    "            coord_scale.append(max_value[j]-center[j])\n",
    "        scale.append(coord_scale)\n",
    "    return scale\n",
    "input_scale = np.asarray(norm_uniform(dataset,2)).squeeze()\n",
    "output_scale = np.asarray(norm_uniform(groundtruth)).squeeze()\n",
    "print(input_scale.shape, output_scale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each axis I normalize the keypoints using the following formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.01948498281535958, 0.5615174968027089) (0.008585920727331952, 0.7395481843884895)\n"
     ]
    }
   ],
   "source": [
    "def normalize(tensor, coordinates=1):\n",
    "    mean_value = [np.nanmean(tensor[:, :,i::coordinates]) for i in range(coordinates)]\n",
    "    std_value = [np.nanstd(tensor[:, :,i::coordinates]) for i in range(coordinates)]\n",
    "    for j in range(coordinates):\n",
    "        subtensor = tensor[:, :, j::coordinates]\n",
    "        subtensor[:] = np.subtract(subtensor, mean_value[j])\n",
    "        subtensor[:] = np.divide(subtensor, std_value[j])\n",
    "  \n",
    "    return [(mean_value[i], std_value[i]) for i in range(coordinates)]\n",
    "\n",
    "mom_x, mom_y = normalize(dataset, 2)\n",
    "print(mom_x, mom_y)\n",
    "mz, stdz = normalize(groundtruth)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([517, 16, 52]) torch.Size([517, 16, 26]) torch.Size([517])\n",
      "torch.Size([517, 2]) torch.Size([517])\n"
     ]
    }
   ],
   "source": [
    "# From python lists to pytorch tensors.\n",
    "dataset = torch.tensor(np.nan_to_num(dataset), dtype=torch.float32)\n",
    "groundtruth = torch.tensor(np.nan_to_num(groundtruth), dtype=torch.float32)\n",
    "lengths = torch.tensor(lengths, dtype=torch.float32)\n",
    "input_scale = torch.tensor(input_scale, dtype=torch.float32)\n",
    "output_scale = torch.tensor(output_scale, dtype=torch.float32)\n",
    "\n",
    "# Randomly shuffle videos\n",
    "permutation = torch.randperm(dataset.size()[0])\n",
    "dataset, groundtruth, lengths = dataset[permutation], groundtruth[permutation], lengths[permutation]\n",
    "input_scale, output_scale = input_scale[permutation], output_scale[permutation]\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "print(input_scale.shape, output_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([383, 16, 52]) torch.Size([67, 16, 52]) torch.Size([67, 16, 52])\n",
      "torch.Size([383, 16, 26]) torch.Size([67, 16, 26]) torch.Size([67, 16, 26])\n",
      "torch.Size([383]) torch.Size([67]) torch.Size([67])\n"
     ]
    }
   ],
   "source": [
    "l1, l2 = len(dataset), len(groundtruth)\n",
    "# Split in train, validation and test\n",
    "training_kp, val_kp, test_kp = dataset[:round(0.74*l1)], dataset[round(0.74*l1):round(0.87*l1)], dataset[round(0.87*l1):]\n",
    "training_lbl, val_lbl, test_lbl = groundtruth[:round(0.74*l2)], groundtruth[round(0.74*l2):round(0.87*l2)], groundtruth[round(0.87*l2):]\n",
    "training_lengths, val_lengths, test_lengths = lengths[:round(0.74*l1)], lengths[round(0.74*l1):round(0.87*l1)], lengths[round(0.87*l1):]\n",
    "training_inpscale, val_inpscale, test_inpscale = input_scale[:round(0.74*l1)], input_scale[round(0.74*l1):round(0.87*l1)], input_scale[round(0.87*l1):]\n",
    "training_outscale, val_outscale, test_outscale = output_scale[:round(0.74*l1)], output_scale[round(0.74*l1):round(0.87*l1)], output_scale[round(0.87*l1):]\n",
    "\n",
    "print(training_kp.shape, val_kp.shape, test_kp.shape)\n",
    "print(training_lbl.shape, val_lbl.shape, test_lbl.shape)\n",
    "print(training_lengths.shape, val_lengths.shape, test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f0eaef9b510>\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(training_kp, training_lbl, training_lengths,\n",
    "                          training_inpscale, training_outscale)\n",
    "val_data = TensorDataset(val_kp, val_lbl, val_lengths,\n",
    "                        val_inpscale, val_outscale)\n",
    "test_data = TensorDataset(test_kp, test_lbl, test_lengths,\n",
    "                         test_inpscale, test_outscale)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the batch_size and put the datasets in DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a GPU available we set our device to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some examples to see whether it is loaded correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 52]) torch.Size([32, 16, 26]) torch.Size([32]) torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y, sample_len, iscale, oscale = dataiter.next()\n",
    "\n",
    "print(sample_x.shape, sample_y.shape, sample_len.shape, iscale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "It is time to build the model for this approach. It will consist on a single/double layer LSTM followed by a Linear layer with output size the number of keypoints we want to estimate. I also define a method to initialize the hidden_state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_2D3D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, bidirectional, dropout=0.):\n",
    "        super().__init__()\n",
    "        # Save the model parameters\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi = bidirectional\n",
    "        \n",
    "        # Define the architecture\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*(2 if self.bi else 1), output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, state, lengths):\n",
    "        # Describe the forward step\n",
    "        batch_size, seq_len = x.size(0), x.size(1) # We save the batch size and the (maximum) sequence length\n",
    "        \n",
    "        # Need to pack a tensor containing padded sequences of variable length\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        ht, hidden_state = self.lstm(packed, state) # ht will be a PackedSequence\n",
    "\n",
    "        # Need to flatten and reshape the output to feed it to the Linear layer\n",
    "        ht = ht.data.contiguous() # ht will be of shape [sum(lengths), hidden_dim]\n",
    "        ot = self.fc(ht) # ot will be of shape [sum(lengths), ouput_size]\n",
    "\n",
    "        l_ot = [ot[:int(length)] for length in lengths] # list of batch elements, each shape [lengths[i], output_size]\n",
    "        packed_ot = nn.utils.rnn.pack_sequence(l_ot, enforce_sorted=False) # PackedSequence\n",
    "        # Finally return to shape [batch_size, seq_len, output_size]\n",
    "        ot, _ = nn.utils.rnn.pad_packed_sequence(packed_ot, batch_first=True, total_length=seq_len)\n",
    "        \n",
    "        return ot, hidden_state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2D3D(\n",
      "  (lstm): LSTM(52, 512, num_layers=2, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "3273754\n"
     ]
    }
   ],
   "source": [
    "# Define some model parameters\n",
    "INPUT_SIZE = sample_x.size(2)\n",
    "OUTPUT_SIZE = sample_y.size(2)\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTM_2D3D(INPUT_SIZE, OUTPUT_SIZE, HIDDEN_DIM, N_LAYERS, BIDIRECTIONAL, dropout=0.)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will proceed with the training. The first cell will define the learning rate, the loss function and the selected optimizer for the training process. Then we will proceed with a training over a number of epochs in which we will print it's training loss and validation loss. I also will be using Tensorboard to have a much nicer view of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_root_PJPE(output):\n",
    "    jb = torch.chunk(output, max_seq, dim=1)\n",
    "    root = 8\n",
    "    n_joints = []\n",
    "    for chunk in jb:\n",
    "        n_joints.append(chunk.sub(chunk[:,:,root].unsqueeze(2)))\n",
    "    joints_merged = torch.cat(tuple(n_joints), dim=1)\n",
    "    return joints_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpjpe(rooted_o, rooted_l, seq_lens):\n",
    "    MPJPE = []\n",
    "    for i in range(len(seq_lens)):\n",
    "        MPJPE.append(rooted_o[i,:int(seq_lens[i])].sub(rooted_l[i,:int(seq_lens[i])]).abs().mean().item())\n",
    "    \n",
    "    return np.mean(MPJPE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "lr = 1e-4\n",
    "loss_function = nn.MSELoss()\n",
    "one_cycle = True\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "if one_cycle:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n",
    "                                                  steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS,\n",
    "                                                  div_factor=25.0, final_div_factor=10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "name = 'body_whole'\n",
    "writer = SummaryWriter(log_dir=f'/deeplearning/logs/{name}{datetime.now()}_lr-{lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/60 in 0.38s.\n",
      " Loss: 1.0010  Val Loss: 0.9944\n",
      "Train MPJPE: 0.5754  Val MPJPE: 0.5701\n",
      "Finished epoch 2/60 in 0.35s.\n",
      " Loss: 0.9927  Val Loss: 0.9878\n",
      "Train MPJPE: 0.5710  Val MPJPE: 0.5676\n",
      "Finished epoch 3/60 in 0.36s.\n",
      " Loss: 0.9798  Val Loss: 0.9747\n",
      "Train MPJPE: 0.5686  Val MPJPE: 0.5637\n",
      "Finished epoch 4/60 in 0.37s.\n",
      " Loss: 0.9597  Val Loss: 0.9586\n",
      "Train MPJPE: 0.5618  Val MPJPE: 0.5555\n",
      "Finished epoch 5/60 in 0.35s.\n",
      " Loss: 0.9328  Val Loss: 0.9362\n",
      "Train MPJPE: 0.5488  Val MPJPE: 0.5445\n",
      "Finished epoch 6/60 in 0.36s.\n",
      " Loss: 0.8828  Val Loss: 0.8900\n",
      "Train MPJPE: 0.5292  Val MPJPE: 0.5289\n",
      "Finished epoch 7/60 in 0.36s.\n",
      " Loss: 0.7783  Val Loss: 0.7884\n",
      "Train MPJPE: 0.4911  Val MPJPE: 0.4814\n",
      "Finished epoch 8/60 in 0.38s.\n",
      " Loss: 0.4968  Val Loss: 0.5835\n",
      "Train MPJPE: 0.3502  Val MPJPE: 0.3496\n",
      "Finished epoch 9/60 in 0.36s.\n",
      " Loss: 0.3780  Val Loss: 0.5878\n",
      "Train MPJPE: 0.2571  Val MPJPE: 0.3554\n",
      "Finished epoch 10/60 in 0.36s.\n",
      " Loss: 0.3182  Val Loss: 0.6122\n",
      "Train MPJPE: 0.2072  Val MPJPE: 0.3610\n",
      "Finished epoch 11/60 in 0.36s.\n",
      " Loss: 0.3098  Val Loss: 0.5601\n",
      "Train MPJPE: 0.2043  Val MPJPE: 0.3455\n",
      "Finished epoch 12/60 in 0.36s.\n",
      " Loss: 0.3027  Val Loss: 0.5646\n",
      "Train MPJPE: 0.2003  Val MPJPE: 0.3473\n",
      "Finished epoch 13/60 in 0.36s.\n",
      " Loss: 0.3119  Val Loss: 0.5770\n",
      "Train MPJPE: 0.2024  Val MPJPE: 0.3491\n",
      "Finished epoch 14/60 in 0.36s.\n",
      " Loss: 0.2994  Val Loss: 0.5556\n",
      "Train MPJPE: 0.1972  Val MPJPE: 0.3417\n",
      "Finished epoch 15/60 in 0.36s.\n",
      " Loss: 0.3051  Val Loss: 0.5503\n",
      "Train MPJPE: 0.2002  Val MPJPE: 0.3451\n",
      "Finished epoch 16/60 in 0.36s.\n",
      " Loss: 0.3041  Val Loss: 0.5413\n",
      "Train MPJPE: 0.1968  Val MPJPE: 0.3356\n",
      "Finished epoch 17/60 in 0.36s.\n",
      " Loss: 0.2957  Val Loss: 0.5520\n",
      "Train MPJPE: 0.1967  Val MPJPE: 0.3428\n",
      "Finished epoch 18/60 in 0.36s.\n",
      " Loss: 0.3013  Val Loss: 0.5455\n",
      "Train MPJPE: 0.1961  Val MPJPE: 0.3433\n",
      "Finished epoch 19/60 in 0.36s.\n",
      " Loss: 0.2983  Val Loss: 0.5326\n",
      "Train MPJPE: 0.1989  Val MPJPE: 0.3269\n",
      "Finished epoch 20/60 in 0.36s.\n",
      " Loss: 0.2995  Val Loss: 0.5212\n",
      "Train MPJPE: 0.1959  Val MPJPE: 0.3375\n",
      "Finished epoch 21/60 in 0.38s.\n",
      " Loss: 0.3016  Val Loss: 0.5245\n",
      "Train MPJPE: 0.1976  Val MPJPE: 0.3243\n",
      "Finished epoch 22/60 in 0.36s.\n",
      " Loss: 0.2979  Val Loss: 0.5037\n",
      "Train MPJPE: 0.1964  Val MPJPE: 0.3171\n",
      "Finished epoch 23/60 in 0.36s.\n",
      " Loss: 0.2968  Val Loss: 0.4839\n",
      "Train MPJPE: 0.1956  Val MPJPE: 0.3107\n",
      "Finished epoch 24/60 in 0.36s.\n",
      " Loss: 0.2918  Val Loss: 0.4543\n",
      "Train MPJPE: 0.1929  Val MPJPE: 0.2988\n",
      "Finished epoch 25/60 in 0.36s.\n",
      " Loss: 0.2851  Val Loss: 0.4640\n",
      "Train MPJPE: 0.1906  Val MPJPE: 0.3096\n",
      "Finished epoch 26/60 in 0.36s.\n",
      " Loss: 0.2868  Val Loss: 0.4368\n",
      "Train MPJPE: 0.1910  Val MPJPE: 0.2930\n",
      "Finished epoch 27/60 in 0.36s.\n",
      " Loss: 0.2771  Val Loss: 0.4161\n",
      "Train MPJPE: 0.1885  Val MPJPE: 0.2818\n",
      "Finished epoch 28/60 in 0.36s.\n",
      " Loss: 0.2738  Val Loss: 0.3996\n",
      "Train MPJPE: 0.1854  Val MPJPE: 0.2740\n",
      "Finished epoch 29/60 in 0.36s.\n",
      " Loss: 0.2762  Val Loss: 0.3806\n",
      "Train MPJPE: 0.1866  Val MPJPE: 0.2578\n",
      "Finished epoch 30/60 in 0.36s.\n",
      " Loss: 0.2689  Val Loss: 0.3598\n",
      "Train MPJPE: 0.1826  Val MPJPE: 0.2546\n",
      "Finished epoch 31/60 in 0.36s.\n",
      " Loss: 0.2689  Val Loss: 0.3553\n",
      "Train MPJPE: 0.1834  Val MPJPE: 0.2483\n",
      "Finished epoch 32/60 in 0.36s.\n",
      " Loss: 0.2616  Val Loss: 0.3460\n",
      "Train MPJPE: 0.1791  Val MPJPE: 0.2413\n",
      "Finished epoch 33/60 in 0.36s.\n",
      " Loss: 0.2605  Val Loss: 0.3158\n",
      "Train MPJPE: 0.1779  Val MPJPE: 0.2281\n",
      "Finished epoch 34/60 in 0.37s.\n",
      " Loss: 0.2561  Val Loss: 0.3063\n",
      "Train MPJPE: 0.1776  Val MPJPE: 0.2221\n",
      "Finished epoch 35/60 in 0.36s.\n",
      " Loss: 0.2552  Val Loss: 0.2948\n",
      "Train MPJPE: 0.1725  Val MPJPE: 0.2092\n",
      "Finished epoch 36/60 in 0.36s.\n",
      " Loss: 0.2566  Val Loss: 0.2926\n",
      "Train MPJPE: 0.1745  Val MPJPE: 0.2038\n",
      "Finished epoch 37/60 in 0.36s.\n",
      " Loss: 0.2567  Val Loss: 0.2896\n",
      "Train MPJPE: 0.1752  Val MPJPE: 0.1982\n",
      "Finished epoch 38/60 in 0.36s.\n",
      " Loss: 0.2501  Val Loss: 0.2891\n",
      "Train MPJPE: 0.1696  Val MPJPE: 0.1936\n",
      "Finished epoch 39/60 in 0.36s.\n",
      " Loss: 0.2467  Val Loss: 0.2753\n",
      "Train MPJPE: 0.1692  Val MPJPE: 0.1899\n",
      "Finished epoch 40/60 in 0.35s.\n",
      " Loss: 0.2470  Val Loss: 0.2697\n",
      "Train MPJPE: 0.1692  Val MPJPE: 0.1819\n",
      "Finished epoch 41/60 in 0.36s.\n",
      " Loss: 0.2476  Val Loss: 0.2652\n",
      "Train MPJPE: 0.1685  Val MPJPE: 0.1772\n",
      "Finished epoch 42/60 in 0.36s.\n",
      " Loss: 0.2453  Val Loss: 0.2506\n",
      "Train MPJPE: 0.1665  Val MPJPE: 0.1729\n",
      "Finished epoch 43/60 in 0.36s.\n",
      " Loss: 0.2509  Val Loss: 0.2696\n",
      "Train MPJPE: 0.1675  Val MPJPE: 0.1806\n",
      "Finished epoch 44/60 in 0.35s.\n",
      " Loss: 0.2462  Val Loss: 0.2673\n",
      "Train MPJPE: 0.1674  Val MPJPE: 0.1764\n",
      "Finished epoch 45/60 in 0.36s.\n",
      " Loss: 0.2420  Val Loss: 0.2599\n",
      "Train MPJPE: 0.1657  Val MPJPE: 0.1741\n",
      "Finished epoch 46/60 in 0.36s.\n",
      " Loss: 0.2478  Val Loss: 0.2654\n",
      "Train MPJPE: 0.1686  Val MPJPE: 0.1718\n",
      "Finished epoch 47/60 in 0.36s.\n",
      " Loss: 0.2388  Val Loss: 0.2561\n",
      "Train MPJPE: 0.1656  Val MPJPE: 0.1707\n",
      "Finished epoch 48/60 in 0.38s.\n",
      " Loss: 0.2475  Val Loss: 0.2653\n",
      "Train MPJPE: 0.1667  Val MPJPE: 0.1729\n",
      "Finished epoch 49/60 in 0.36s.\n",
      " Loss: 0.2464  Val Loss: 0.2493\n",
      "Train MPJPE: 0.1676  Val MPJPE: 0.1686\n",
      "Finished epoch 50/60 in 0.36s.\n",
      " Loss: 0.2442  Val Loss: 0.2687\n",
      "Train MPJPE: 0.1660  Val MPJPE: 0.1793\n",
      "Finished epoch 51/60 in 0.36s.\n",
      " Loss: 0.2472  Val Loss: 0.2633\n",
      "Train MPJPE: 0.1673  Val MPJPE: 0.1695\n",
      "Finished epoch 52/60 in 0.35s.\n",
      " Loss: 0.2493  Val Loss: 0.2601\n",
      "Train MPJPE: 0.1676  Val MPJPE: 0.1709\n",
      "Finished epoch 53/60 in 0.36s.\n",
      " Loss: 0.2439  Val Loss: 0.2647\n",
      "Train MPJPE: 0.1653  Val MPJPE: 0.1729\n",
      "Finished epoch 54/60 in 0.36s.\n",
      " Loss: 0.2458  Val Loss: 0.2649\n",
      "Train MPJPE: 0.1650  Val MPJPE: 0.1710\n",
      "Finished epoch 55/60 in 0.36s.\n",
      " Loss: 0.2431  Val Loss: 0.2595\n",
      "Train MPJPE: 0.1654  Val MPJPE: 0.1709\n",
      "Finished epoch 56/60 in 0.36s.\n",
      " Loss: 0.2493  Val Loss: 0.2488\n",
      "Train MPJPE: 0.1662  Val MPJPE: 0.1692\n",
      "Finished epoch 57/60 in 0.36s.\n",
      " Loss: 0.2461  Val Loss: 0.2469\n",
      "Train MPJPE: 0.1661  Val MPJPE: 0.1664\n",
      "Finished epoch 58/60 in 0.35s.\n",
      " Loss: 0.2474  Val Loss: 0.2587\n",
      "Train MPJPE: 0.1655  Val MPJPE: 0.1666\n",
      "Finished epoch 59/60 in 0.35s.\n",
      " Loss: 0.2489  Val Loss: 0.2612\n",
      "Train MPJPE: 0.1656  Val MPJPE: 0.1727\n",
      "Finished epoch 60/60 in 0.35s.\n",
      " Loss: 0.2464  Val Loss: 0.2633\n",
      "Train MPJPE: 0.1661  Val MPJPE: 0.1700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75147389868b4fc892d28c97ce6d7685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0e8028ec10>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer_beg = timer()\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "model.train()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Init the hidden state (ht, ct)\n",
    "    h = model.init_hidden(batch_size)\n",
    "    batch_losses = []\n",
    "    train_MPJPE = []\n",
    "    \n",
    "    if i+1 == NUM_EPOCHS:\n",
    "        preds, inps, labls, lens = [], [], [], []\n",
    "        val_preds, val_inps, val_labls, val_lens = [], [], [], []\n",
    "        iscale, oscale, val_iscale, val_oscale = [], [], [], []\n",
    "        \n",
    "    for inputs, labels, lengths, i_s, o_s in train_loader:\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward step\n",
    "        output, h = model(inputs, h, lengths)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            preds.append(output)\n",
    "            inps.append(inputs)\n",
    "            labls.append(labels)\n",
    "            lens.append(lengths)\n",
    "            iscale.append(i_s)\n",
    "            oscale.append(o_s)\n",
    "\n",
    "        # Loss calculation and backward step\n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(output, lengths=lengths, batch_first=True, \n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels, lengths=lengths, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        loss.backward()\n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "        # One cycle policy step\n",
    "        if one_cycle:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Output data collection for showing\n",
    "        batch_losses.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(output), substract_root_PJPE(labels)\n",
    "        train_MPJPE.append(mpjpe(rooted_o, rooted_l, lengths))\n",
    "    \n",
    "    timer_end = timer()\n",
    "    tr_losses.append(np.mean(batch_losses))\n",
    "    writer.add_scalar('Loss/train', tr_losses[-1], i)   \n",
    "    train_MPJPE_total = np.mean(train_MPJPE)\n",
    "    \n",
    "    # Validation at the end of an epoch\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_MPJPE = []\n",
    "    \n",
    "    for inp, lab, lns, vis, vos in val_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        inp, lab, lns = inp.to(device), lab.to(device), lns.to(device)\n",
    "        out, val_h = model(inp, val_h, lns)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            val_preds.append(output)\n",
    "            val_inps.append(inp)\n",
    "            val_labls.append(lab)\n",
    "            val_lens.append(lns)\n",
    "            val_iscale.append(vis)\n",
    "            val_oscale.append(vos)\n",
    "        \n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(out, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(lab, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        val_loss.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(out), substract_root_PJPE(lab)\n",
    "        val_MPJPE.append(mpjpe(rooted_o, rooted_l, lns))\n",
    "    \n",
    "    val_losses.append(np.mean(val_loss))\n",
    "    writer.add_scalar('Loss/validation', val_losses[-1], i)  \n",
    "    val_MPJPE_total = np.mean(val_MPJPE)\n",
    "    model.train()\n",
    "    \n",
    "    # Output loss and training time.\n",
    "    print(f\"Finished epoch {i+1}/{NUM_EPOCHS} in {(timer_end-timer_beg):.2f}s.\\n\",\n",
    "             f\"Loss: {np.mean(tr_losses[-1]):.4f}\",\n",
    "             f\" Val Loss: {val_losses[-1]:.4f}\\n\"\n",
    "             f\"Train MPJPE: {train_MPJPE_total*stdz:.4f}\", f\" Val MPJPE: {val_MPJPE_total*stdz:.4f}\")\n",
    "    timer_beg = timer()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_losses, label='train')\n",
    "plt.plot(val_losses, label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSELoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([352, 2]) torch.Size([352, 16, 52])\n"
     ]
    }
   ],
   "source": [
    "tr_predictions = torch.cat(tuple(preds), dim=0)\n",
    "tr_inputs = torch.cat(tuple(inps), dim=0)\n",
    "tr_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "tr_lengths = torch.cat(tuple(lens), dim=0)\n",
    "tr_inp_scale, tr_out_scale = torch.cat(tuple(iscale), dim=0), torch.cat(tuple(oscale), dim=0)\n",
    "print(tr_inp_scale.shape, tr_inputs.shape)\n",
    "\n",
    "val_predictions = torch.cat(tuple(val_preds), dim=0)\n",
    "val_inputs = torch.cat(tuple(val_inps), dim=0)\n",
    "val_groundtruth = torch.cat(tuple(val_labls), dim=0)\n",
    "val_length = torch.cat(tuple(val_lens), dim=0)\n",
    "val_inp_scale, val_out_scale = torch.cat(tuple(val_iscale), dim=0), torch.cat(tuple(val_oscale), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros((26), device='cuda:0')\n",
    "\n",
    "for i in range(len(tr_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(tr_lengths[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(tr_lengths[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n",
      "0/16\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros((26), device='cuda:0')\n",
    "\n",
    "for i in range(len(val_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(val_length[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(val_length[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5155211199565655\n"
     ]
    }
   ],
   "source": [
    "print(stdz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./{name}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "After the training, we shall proceed with the performance test. This will go through the test batches and perform the inference, then it will show the test loss, as well as the performance metric. In this case, as we are working with human body keypoints, we will use the Mean Per Joint Position Error (MPJPE) metric, which outputs the mean euclidean distance between the joints (keypoints) positions estimated and the ones in the groundtruth.\n",
    "\n",
    "The formula for MPJPE is the following:\n",
    "\n",
    "$\\text{MPJPE} = \\frac1T\\frac1N\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{N}\\|(J_{i}^{(t)}-J_{root}^{(t)})-(Ä´_{i}^{(t)}-Ä´_{root}^{(t)})\\|$\n",
    "\n",
    "Where N is the number of joints, and T the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the formula above, we need to align the root joints of the labels and the network output. In order to do that, I have defined a function (`substract_root_PJPE`) that substracts the root joint of each keypoint set (face, hands, body) in the corresponding keypoint set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "MPJPE = []\n",
    "h = model.init_hidden(batch_size)\n",
    "preds, inps, labls, lengs = [], [], [], []\n",
    "iscal, oscal = [], []\n",
    "\n",
    "model.eval()\n",
    "for inputs_test, labels_test, lengths_test, is_test, os_test in test_loader:\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs_test, labels_test, lengths_test = inputs_test.to(device), labels_test.to(device), lengths_test.to(device)\n",
    "    \n",
    "    output_test, h = model(inputs_test, h, lengths_test)\n",
    "    preds.append(output_test)\n",
    "    inps.append(inputs_test)\n",
    "    labls.append(labels_test)\n",
    "    lengs.append(lengths_test)\n",
    "    iscal.append(is_test)\n",
    "    oscal.append(os_test)\n",
    "    \n",
    "    test_loss = loss_function(nn.utils.rnn.pack_padded_sequence(output_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "    test_losses.append(test_loss.item())\n",
    "    rooted_o, rooted_l = substract_root_PJPE(output_test), substract_root_PJPE(labels_test)\n",
    "    MPJPE.append(mpjpe(rooted_o, rooted_l, lengths_test))\n",
    "\n",
    "test_predictions = torch.cat(tuple(preds), dim=0)\n",
    "test_inputs = torch.cat(tuple(inps), dim=0)\n",
    "test_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "test_inp_scale, test_out_scale = torch.cat(tuple(iscal), dim=0), torch.cat(tuple(oscal), dim=0)\n",
    "MPJPE_total = np.mean(MPJPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 0.1708 \n",
      "Test loss: 0.2379\n"
     ]
    }
   ],
   "source": [
    "print(f\"MPJPE: {MPJPE_total*stdz:.4f}\", f\"\\nTest loss: {np.mean(test_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results into a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'train':{'inputs':tr_inputs.tolist(), 'predictions':tr_predictions.tolist(), 'labels':tr_groundtruth.tolist(), 'lengths':tr_lengths.tolist()},\n",
    "          'validation':{'inputs':val_inputs.tolist(), 'predictions':val_predictions.tolist(), 'labels':val_groundtruth.tolist(), 'lengths':val_length.tolist()},\n",
    "          'test':{'inputs':test_inputs.tolist(), 'predictions':test_predictions.tolist(), 'labels':test_groundtruth.tolist(), 'lengths':test_lengths.tolist()}}\n",
    "with open('../../results/small_body.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/small_body.json', 'r') as j:\n",
    "    jd = json.load(j)\n",
    "    tr, val, test = jd['train'], jd['validation'], jd['test']\n",
    "    tr_inputs, tr_predictions, tr_groundtruth, tr_lengths = tuple(torch.tensor(tr[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    val_inputs, val_predictions, val_groundtruth, val_length = tuple(torch.tensor(val[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    test_inputs, test_predictions, test_groundtruth, test_lengths = tuple(torch.tensor(test[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 8752, 52]), torch.Size([8, 8752, 26]), torch.Size([8]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs.shape, val_predictions.shape, test_lengths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to better understanding of the results, I will plot some of the frames from the last batches on the training and validation, and also from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_rotate(c_inputs, c_z, frames, frame):\n",
    "    c_inputs[:,::2].mul_(mom_x[1])\n",
    "    c_inputs[:,1::2].mul_(mom_y[1])\n",
    "    c_z.mul_(stdz)\n",
    "\n",
    "    bodiesXY = torch.chunk(c_inputs[frames, :], len(frames), dim=0)\n",
    "    bodiesZ = torch.chunk(c_z[frames, :], len(frames), dim=0)\n",
    "    \n",
    "    x = bodiesXY[frame].squeeze()[::2]\n",
    "    y = bodiesXY[frame].squeeze()[1::2]\n",
    "    z = bodiesZ[frame].squeeze()\n",
    "\n",
    "    l_arm = [[c[i] for i in [1, 0, 9, 10, 11]] for c in [x,y,z]]\n",
    "    r_arm = [[c[i] for i in [0, 3, 4, 5]] for c in [x,y,z]]\n",
    "    l_leg = [[c[i] for i in [0, 2, 12, 13, 14, 22, 23, 24]] for c in [x,y,z]]\n",
    "    r_leg = [[c[i] for i in [2, 6, 7, 8, 19, 20, 21]] for c in [x,y,z]]\n",
    "    head = [[c[i] for i in [18, 17, 1, 15, 16]] for c in [x,y,z]]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    def init():\n",
    "        ax.plot(r_arm[0], r_arm[2], r_arm[1])\n",
    "        ax.plot(l_arm[0], l_arm[2], l_arm[1])\n",
    "        ax.plot(r_leg[0], r_leg[2], r_leg[1])\n",
    "        ax.plot(l_leg[0], l_leg[2], l_leg[1])\n",
    "        ax.plot(head[0], head[2], head[1])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "        \n",
    "        return fig,\n",
    "\n",
    "    def animate(i):\n",
    "        ax.view_init(elev=220., azim=3.6*i)\n",
    "        return fig,\n",
    "\n",
    "    # Animate\n",
    "    ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=100, blit=True)    \n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(predictions, groundtruth, inputs, video_n, frames, rot):\n",
    "    \n",
    "    inp = inputs.clone()\n",
    "    preds = predictions.clone()\n",
    "    grtr = groundtruth.clone()\n",
    "    \n",
    "    inp[:,:,::2].mul_(mom_x[1])\n",
    "    inp[:,:,1::2].mul_(mom_y[1])\n",
    "    preds.mul_(stdz)\n",
    "    grtr.mul_(stdz)\n",
    "    \n",
    "    bodiesXY = torch.chunk(inp[video_n, frames, :], len(frames), dim=0)\n",
    "    pred_bodiesZ = torch.chunk(preds[video_n, frames, :], len(frames), dim=0)\n",
    "    true_bodiesZ = torch.chunk(grtr[video_n, frames, :], len(frames), dim=0)\n",
    "   \n",
    "    nrows = np.ceil(len(frames)/2)\n",
    "    fig = plt.figure(figsize=(15, 6*nrows))\n",
    "    fig2 = plt.figure(figsize=(15, 6*nrows))\n",
    "    for frame in range(len(frames)):\n",
    "        x = bodiesXY[frame].squeeze()[::2].tolist()\n",
    "        y = bodiesXY[frame].squeeze()[1::2].tolist()\n",
    "        pred_z = pred_bodiesZ[frame].squeeze().tolist()\n",
    "        true_z = true_bodiesZ[frame].squeeze().tolist()\n",
    "        \n",
    "        r = R.from_euler('y', rot, degrees=True)\n",
    "        \n",
    "        xyz1, xyz2 = np.asarray([c for c in zip(x, y, pred_z)]), np.asarray([c for c in zip(x, y, true_z)])\n",
    "        xyz1, xyz2 = r.apply(xyz1), r.apply(xyz2)\n",
    "        x1, x2 = xyz1[:,0], xyz2[:,0]\n",
    "        y1, y2 = xyz1[:,1], xyz2[:,1]\n",
    "        pred_z, true_z = xyz1[:,2], xyz2[:,2]\n",
    "        \n",
    "        r_arm = tuple([[c[i] for i in [1, 0, 9, 10, 11]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        l_arm = tuple([[c[i] for i in [0, 3, 4, 5]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        r_leg = tuple([[c[i] for i in [0, 2, 12, 13, 14, 22, 23, 24]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        l_leg = tuple([[c[i] for i in [2, 6, 7, 8, 19, 20, 21]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        head = tuple([[c[i] for i in [18, 17, 1, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "\n",
    "        ax = fig.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "\n",
    "        ax.plot(r_arm[0][0], r_arm[0][1], r_arm[0][2])\n",
    "        ax.plot(l_arm[0][0], l_arm[0][1], l_arm[0][2])\n",
    "        ax.plot(r_leg[0][0], r_leg[0][1], r_leg[0][2])\n",
    "        ax.plot(l_leg[0][0], l_leg[0][1], l_leg[0][2])\n",
    "        ax.plot(head[0][0], head[0][1], head[0][2])\n",
    "        \n",
    "        ax2 = fig2.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "        ax2.plot(r_arm[1][0], r_arm[1][1], r_arm[1][2])\n",
    "        ax2.plot(l_arm[1][0], l_arm[1][1], l_arm[1][2])\n",
    "        ax2.plot(r_leg[1][0], r_leg[1][1], r_leg[1][2])\n",
    "        ax2.plot(l_leg[1][0], l_leg[1][1], l_leg[1][2])\n",
    "        ax2.plot(head[1][0], head[1][1], head[1][2])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "        \n",
    "        lims2 = ax2.get_xlim(), ax2.get_ylim(), ax2.get_zlim()\n",
    "        spans2 = lims2[0][1]-lims2[0][0], lims2[1][1]-lims2[1][0], lims2[2][1]-lims2[2][0]\n",
    "        span2 = max(spans2)\n",
    "        margins2 = [(span2-s)/2 for  s in spans2]\n",
    "        ax2.set_xlim(lims2[0][0]-margins2[0], lims2[0][1]+margins2[0])\n",
    "        ax2.set_ylim(lims2[1][0]-margins2[1], lims2[1][1]+margins2[1])\n",
    "        ax2.set_zlim(lims2[2][0]-margins2[2], lims2[2][1]+margins2[2])\n",
    "\n",
    "        ax.view_init(elev=-65., azim=-90.)\n",
    "        ax2.view_init(elev=-65., azim=-90.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single frame\n",
    "On the first cell you can select which frames you want to plot and from which video of the batch. On the second you select which frame of the previosly selected you want to plot, specifying its index on the declared \"frames\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last batches of training -output, inputs, labels-.\n",
    "vid = 1\n",
    "frames = [100]\n",
    "\n",
    "c_inputs = training_kp[vid].clone()\n",
    "c_labels = training_lbl[vid].clone()\n",
    "c_inputs[:,::2].mul_(training_inpscale[vid, 0])\n",
    "c_inputs[:,1::2].mul_(training_inpscale[vid, 1])\n",
    "c_labels.mul_(training_outscale[vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(plot_and_rotate(c_inputs, c_output, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs = tr_inputs[vid].clone()\n",
    "HTML(plot_and_rotate(c_inputs, c_labels, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice of frames\n",
    "Now let's plot a sequence of frames of the selected video. We will plot both the groundtruth and the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,7)]\n",
    "video_n = 6\n",
    "\n",
    "c_inputs = tr_inputs.clone()\n",
    "c_output = tr_predictions.clone()\n",
    "c_labels = tr_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(tr_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(tr_out_scale[vid])\n",
    "    c_labels[vid].mul_(tr_out_scale[vid])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f284de0d905d422aa3b00112a1fe3abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924bd806eba543bfbdafc4b31949b172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,7)]\n",
    "video_n = 2\n",
    "\n",
    "c_inputs = test_inputs.clone()\n",
    "c_output = test_predictions.clone()\n",
    "c_labels = test_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(test_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(test_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(test_out_scale[vid])\n",
    "    c_labels[vid].mul_(test_out_scale[vid])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb2068652e74e0d92d2c5485890e005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab84d9a4e0684df6bb677a00fa2df516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
